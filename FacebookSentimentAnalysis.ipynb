{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and import the Natural Language Toolkit.\n",
    "\n",
    "#pip install nltk \n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "stop_words = stopwords.words('english') ## We want to use English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dpwnload the Punkt package for tokenising.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## Download wordnet and averaged perceptron trigger for normalising/lemmatising the text. Import pos_tag to determine word context.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download the 'twitter_samples' dataset and stop words.\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the packages we need to remove noise.\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import random to shuffle.\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import classify and Naive Bayes for the model.\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Beautiful Soup to scrape Facbook messages\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create variables for the positive tweets, negative tweets and text\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenise the tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising and Removing Noise ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to lemmatize the tweets and remove noise. This reduces words to their root form and gets rid of links and useless words. \n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = [] # Initialise the output vecotr\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub(r'^https?:\\/\\/.*[\\r\\n]*','', token) # Removes links.\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token) # Removes @ mentions.\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n' # Assign nouns as nouns.\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v' ## Assign verbs as verbs.\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer() # Define lemmatizer as the wordnet lemmatizer.\n",
    "        token = lemmatizer.lemmatize(token, pos) # Reduce the words to their root words.\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words: # Gets rid of the empty tweets and any punctuation.\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) ## Clean the positive tweets.\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) ## Clean the negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Density ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to return a list of all the words from tweets.\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list: # For every tweet in the defined list.\n",
    "        for token in tokens: # AND for every word in every element of the list.\n",
    "            yield token # Return the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the above function to return all positive and negative tweets words.\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a frequency distribution to find most common words.\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modelling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the values to a dictionary for us in naive bayes classification.\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the lists of model ready data.\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data in to train and test data.\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model] # Label the positive data.\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model] # Label the negative data.\n",
    "\n",
    "dataset = positive_dataset + negative_dataset # Combine the data sets.\n",
    "\n",
    "random.shuffle(dataset) # Shuffle the data so theres no natural ordering.\n",
    "\n",
    "train_data = dataset[:7000] # First 7000 entries for train.\n",
    "test_data = dataset[7000:] # Final 3000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model!\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9933333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2035.8 : 1.0\n",
      "                      :) = True           Positi : Negati =   1669.1 : 1.0\n",
      "                  arrive = True           Positi : Negati =     29.1 : 1.0\n",
      "                followed = True           Negati : Positi =     26.3 : 1.0\n",
      "                     sad = True           Negati : Positi =     24.9 : 1.0\n",
      "                follower = True           Positi : Negati =     19.1 : 1.0\n",
      "               community = True           Positi : Negati =     18.1 : 1.0\n",
      "                     via = True           Positi : Negati =     16.3 : 1.0\n",
      "                 welcome = True           Positi : Negati =     15.4 : 1.0\n",
      "                    luck = True           Positi : Negati =     14.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Return the accuracy and the words that are most useful in determing the sentiment.\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "## Test out the model with custom tweets!\n",
    "custom_tweet = \"Test message\"\n",
    "custom_tokens = remove_noise(nltk.word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model on Facebook messages #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Facebook messages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise the file to scrape.\n",
    "url = 'D:\\User\\*File_Location*' + str(1) + '.html'\n",
    "soup = BeautifulSoup(open(url, encoding = 'utf8').read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise all pages of the scrape.\n",
    "for i in range(1,5):\n",
    "    url = 'D:\\User\\*File_Location*' + str(i) + '.html' # Location to scrape from.\n",
    "    page = open(url, encoding = 'utf8') # Open the file with utf8 encoding.\n",
    "    soup[i] = BeautifulSoup(page.read(), 'html.parser') # Create a BS object to scrape from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape the messages.\n",
    "Texts = [] # Initialise the Texts vector.\n",
    "j = 0\n",
    "for i in range(1,5): # Iterates over every page.\n",
    "    for div in soup[i].find_all('div', class_ = '_3-96 _2let'): ## Returns the message.\n",
    "        Texts.append(div.text) ## Saves output to Texts\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texter = []\n",
    "for i in range(1,5):\n",
    "    for div in soup[i].find_all('div', class_ = '_3-96 _2pio _2lek _2lel'): ## Returns the person that sent the text\n",
    "        Texter.append(div.text) ## Saves output to Texter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_date = []\n",
    "for i in range(1,5):\n",
    "    for div in soup[i].find_all('div', class_ = '_3-94 _2lem'): ## Returns the person that sent the text\n",
    "        mess_date.append(div.text) ## Saves output to Texter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the messages through the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = []\n",
    "for msg in Texts:\n",
    "    text_tokens.append(nltk.word_tokenize(msg)) # Tokenize the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in text_tokens:\n",
    "    txt_cleaned_tokens_list.append(remove_noise(tokens, stop_words)) ## Clean the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_analysis = get_tweets_for_model(Texts)\n",
    "\n",
    "msg_clas = []\n",
    "for txts in texts_for_analysis:\n",
    "    msg_clas.append(classifier.classify(dict([token, True] for token in txts))) ## Run the messages through a calssifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTexts = list(zip(Texter,Texts, mess_date, msg_clas))\n",
    "Mess_df = pd.DataFrame(FullTexts, columns = ['Texter','Texts', 'mess_date', 'msg_clas']) # Add the sentiment to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the number of positive and negative messages.\n",
    "\n",
    "num_pos_msg = 0\n",
    "for msg in msg_clas:\n",
    "    if msg == 'Positive':\n",
    "        num_pos_msg += 1\n",
    "\n",
    "num_neg_msg = 0\n",
    "for msg in msg_clas:\n",
    "    if msg == 'Negative':\n",
    "        num_neg_msg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.18524183235965"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pos_msg/(num_pos_msg + num_neg_msg)*100 # Determine percentage of positive messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mess_df.to_csv('Messages_Sentiment.csv') # Svae the file to a csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
